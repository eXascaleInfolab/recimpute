{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b73b1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:52:28.221688Z",
     "start_time": "2022-04-29T15:52:28.221674Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "from IPython.display import display, HTML\n",
    "import itertools\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numexpr as ne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import random as rdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36853847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:28:48.239697Z",
     "start_time": "2022-04-29T15:28:48.204533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/guillaume/recimpute\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5deaa169",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:27:17.574065Z",
     "start_time": "2022-04-29T15:27:17.534818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from recimpute import init_training_set, FEATURES_EXTRACTORS\n",
    "from Datasets.Dataset import Dataset\n",
    "from Clustering.ShapeBasedClustering import ShapeBasedClustering\n",
    "from Labeling.ImputationTechniques.ImputeBenchLabeler import ImputeBenchLabeler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcdbdf3",
   "metadata": {},
   "source": [
    "## Compute profile vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83931b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:21:03.620262Z",
     "start_time": "2022-04-29T15:21:03.583221Z"
    }
   },
   "outputs": [],
   "source": [
    "MODE = 'eval' # 'use' or 'eval'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22baf5d",
   "metadata": {},
   "source": [
    "### Profile vectors for already labeled time series / clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7d658b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:21:04.477318Z",
     "start_time": "2022-04-29T15:21:04.445468Z"
    }
   },
   "outputs": [],
   "source": [
    "ERROR = 'average rank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702213ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:27:43.210540Z",
     "start_time": "2022-04-29T15:27:19.277613Z"
    }
   },
   "outputs": [],
   "source": [
    "CLUSTERER = ShapeBasedClustering()\n",
    "DATASETS = Dataset.instantiate_from_dir(CLUSTERER)\n",
    "LABELER = ImputeBenchLabeler.get_instance()\n",
    "LABELER_PROPERTIES = LABELER.get_default_properties()\n",
    "FEATURES_EXTRACTOR_init = [fe.get_instance() for fe in FEATURES_EXTRACTORS.values()]\n",
    "training_set = init_training_set(LABELER, LABELER_PROPERTIES, None, None, FEATURES_EXTRACTOR_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d27ea3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:25:01.816917Z",
     "start_time": "2022-04-29T15:22:21.729124Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data_info, labels_set = training_set._load(data_to_load= 'all' if MODE == 'use' else 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3472b84d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:25:01.852400Z",
     "start_time": "2022-04-29T15:25:01.819232Z"
    }
   },
   "outputs": [],
   "source": [
    "all_cids = all_data_info['Cluster ID'].unique()\n",
    "existing_vectors = pd.DataFrame(index=sorted(all_cids), columns=['Features Vector', 'Benchmark Results'])\n",
    "existing_vectors.index.name = 'Cluster ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be03e5d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:25:01.877854Z",
     "start_time": "2022-04-29T15:25:01.853918Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_ds_name_from_cid(cid, datasets):\n",
    "    for ds in datasets:\n",
    "        if cid in ds.cids:\n",
    "            return ds.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26c78200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:25:01.904648Z",
     "start_time": "2022-04-29T15:25:01.879936Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_cluster_bench_res(cid, labeler, properties, datasets):\n",
    "    ds_name = get_ds_name_from_cid(cid, datasets)\n",
    "    # load clusters labels\n",
    "    labels_filename = labeler._get_labels_filename(ds_name)\n",
    "    all_benchmark_results = pd.read_csv(labels_filename, index_col='Cluster ID')\n",
    "    \n",
    "    # identify algorithms to exclude from labels list if some reduction threshold has been specified\n",
    "    algos_to_exclude = labeler._get_algos_to_exclude(all_benchmark_results, properties) \\\n",
    "                        if properties['reduction_threshold'] > 0.0 else []\n",
    "\n",
    "    row = all_benchmark_results.loc[cid]\n",
    "    # convert bench_res to DataFrame\n",
    "    benchmark_results = labeler._convert_bench_res_to_df(row.values[0])\n",
    "    # get a ranked list of algorithms for this cluster (from best to worse)\n",
    "    ranking_strat = ImputeBenchLabeler.CONF['BENCH_RES_AGG_AND_RANK_STRATEGY']\n",
    "    ranked_algos_for_cid = labeler._get_ranking_from_bench_res(\n",
    "        benchmark_results,\n",
    "        ranking_strat=ranking_strat,\n",
    "        ranking_strat_params=ImputeBenchLabeler.CONF['BENCH_RES_AGG_AND_RANK_STRATEGY_PARAMS'][ranking_strat],\n",
    "        error_to_minimize=ImputeBenchLabeler.CONF['BENCHMARK_ERROR_TO_MINIMIZE'],\n",
    "        algos_to_exclude=algos_to_exclude,\n",
    "        return_scores=True\n",
    "    )\n",
    "\n",
    "    return ranked_algos_for_cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "687268bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:35:51.105804Z",
     "start_time": "2022-04-29T15:28:52.990412Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bd996072a94ec39bfcbcd700a00273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features Vector</th>\n",
       "      <th>Benchmark Results</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cluster ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Values__variance_larger_than_standard_deviatio...</td>\n",
       "      <td>weighted average rmse  average rms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Values__variance_larger_than_standard_deviatio...</td>\n",
       "      <td>weighted average rmse  average rms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Values__variance_larger_than_standard_deviatio...</td>\n",
       "      <td>weighted average rmse  average rms...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Features Vector  \\\n",
       "Cluster ID                                                      \n",
       "0           Values__variance_larger_than_standard_deviatio...   \n",
       "1           Values__variance_larger_than_standard_deviatio...   \n",
       "2           Values__variance_larger_than_standard_deviatio...   \n",
       "\n",
       "                                            Benchmark Results  \n",
       "Cluster ID                                                     \n",
       "0                       weighted average rmse  average rms...  \n",
       "1                       weighted average rmse  average rms...  \n",
       "2                       weighted average rmse  average rms...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all existing profile vectors and benchmark results\n",
    "for cid in tqdm(all_cids):\n",
    "    # load cluster's features vector\n",
    "    existing_vectors.at[cid, 'Features Vector'] = all_data_info.loc[all_data_info['Cluster ID'] == cid, \n",
    "                                                                    ~all_data_info.columns.isin(('Data Set Name', \n",
    "                                                                                                 'Label', \n",
    "                                                                                                 'Cluster ID'))].mean()\n",
    "    # load cluster's bench res\n",
    "    ranked_algos = get_cluster_bench_res(cid, LABELER, LABELER_PROPERTIES, DATASETS)\n",
    "    existing_vectors.at[cid, 'Benchmark Results'] = ranked_algos\n",
    "existing_vectors.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3266d06",
   "metadata": {},
   "source": [
    "### Profile vectors for unlabeled time series / clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b23e7487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:52:28.091954Z",
     "start_time": "2022-04-29T15:35:51.107843Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464759c628394ae59d0e65be1505d2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if MODE == 'eval':\n",
    "    test_data = []\n",
    "    features_name = existing_vectors.iloc[0]['Features Vector'].index\n",
    "    test_all_data_info, _ = training_set._load(data_to_load='test')\n",
    "    # for each test entry: load its profile vector and the benchmark results of its cluster\n",
    "    for _, row in tqdm(test_all_data_info.iterrows(), total=test_all_data_info.shape[0]):\n",
    "        # load profile vec\n",
    "        profile_vector = pd.DataFrame(row).T\n",
    "        profile_vector = profile_vector[profile_vector.columns.intersection(features_name)]\n",
    "        profile_vector = profile_vector[features_name]\n",
    "        # load cluster's features vector\n",
    "        cid = row['Cluster ID']\n",
    "        ranked_algos = get_cluster_bench_res(cid, LABELER, LABELER_PROPERTIES, DATASETS)\n",
    "        \n",
    "        test_data.append((profile_vector, ranked_algos, row['Data Set Name']))\n",
    "elif MODE == 'use':\n",
    "    new_sequence = None # put raw sequence to label here\n",
    "    new_profile_vector = FEATURES_EXTRACTER.extract_from_timeseries(pd.DataFrame(new_sequence), 1, len(new_sequence))\n",
    "    features_name = existing_vectors.loc[0, 'Features Vector'].index\n",
    "    new_profile_vector = new_profile_vector[new_profile_vector.columns.intersection(features_name)]\n",
    "    new_profile_vector = new_profile_vector[features_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d5f69",
   "metadata": {},
   "source": [
    "## Similarity search"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3dd1131a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T16:43:21.617886Z",
     "start_time": "2022-04-14T16:43:21.574884Z"
    }
   },
   "source": [
    "# for each labeled sequence (or cluster):\n",
    "#  compute cosine similarity btw the new profile vector and the labeled profile vector\n",
    "#  for each imputation technique in the profile vector's bench results\n",
    "#    get the RMSE of the imputation technique\n",
    "#    compute score[profile vector, imputation technique] = sim(new profile vector; labeled profile vector) x (1/RMSE)\n",
    "#    save the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "910a9142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:52:28.124624Z",
     "start_time": "2022-04-29T15:52:28.093612Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def custom_cosine_distance(a, b):\n",
    "    _custom_cosine_similarity = lambda a,b: sum(map(lambda i: i[0]*i[1], zip(a,b))) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    sim = _custom_cosine_similarity(a,b)\n",
    "    return 1.-0. if np.isnan(sim) else 1.-sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc3588bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:52:28.157579Z",
     "start_time": "2022-04-29T15:52:28.126666Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_recommendations(existing_vectors, new_profile_vector, all_cids, dist_metric, \n",
    "                        get_recommendations_strat='all', norm_error=True):\n",
    "    # initialization\n",
    "    t0 = time.time()\n",
    "    all_techniques = ['cdrec_k2','cdrec_k3','dynammo','grouse','rosl','softimp','svdimp','svt','stmvl','spirit','tenmf','tkcm','trmf']\n",
    "    my_index = pd.MultiIndex.from_tuples(\n",
    "        list(itertools.product(sorted(all_cids), all_techniques)), \n",
    "        names=['Cluster ID', 'Technique']\n",
    "    )\n",
    "    all_scores = pd.DataFrame(index=my_index, columns=['Score', 'Distance', ERROR])\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # compute score for each existing profile vector\n",
    "    if norm_error:\n",
    "        g_max_error = existing_vectors['Benchmark Results'].map(lambda bench_res: bench_res[ERROR].max()).max()\n",
    "        existing_vectors['G Normalized Benchmark Results'] = existing_vectors['Benchmark Results']\\\n",
    "                                                                .map(lambda bench_res: bench_res[ERROR] / g_max_error)\n",
    "\n",
    "    for cid, row in existing_vectors.iterrows():\n",
    "        dist = custom_cosine_distance(\n",
    "            row['Features Vector'].to_numpy(), \n",
    "            new_profile_vector.iloc[0].to_numpy(),\n",
    "        )\n",
    "        for technique in all_techniques:\n",
    "            try:\n",
    "                rmse = row['G Normalized Benchmark Results'].loc[technique] if norm_error else \\\n",
    "                        row['Benchmark Results'][ERROR].loc[technique]\n",
    "                score = dist * rmse\n",
    "                all_scores.at[(cid, technique)] = (score, dist, rmse)\n",
    "            except KeyError:\n",
    "                all_scores.at[(cid, technique)] = (np.inf, np.inf, np.inf)\n",
    "    if norm_error:\n",
    "        del existing_vectors['G Normalized Benchmark Results']\n",
    "    t2 = time.time()\n",
    "    #display(all_scores[~all_scores.isin([np.nan, np.inf, -np.inf]).any(1)]['Distance']) # TODO tmp print\n",
    "    \n",
    "    # minimize the scores\n",
    "    sorted_techniques = all_scores.copy(deep=True).sort_values('Score', ascending=True)\n",
    "    #display(sorted_techniques[:6]) # TODO tmp print\n",
    "    sorted_techniques.index = sorted_techniques.index.droplevel(0)\n",
    "    sorted_techniques = sorted_techniques.groupby('Technique').head(1).sort_values('Score', ascending=True)\n",
    "    t3 = time.time()\n",
    "    \n",
    "    #display(sorted_techniques[:6]) # TODO tmp print\n",
    "    if get_recommendations_strat == 'all':\n",
    "        return sorted_techniques\n",
    "    elif get_recommendations_strat == 'top3':\n",
    "        return sorted_techniques[:3]\n",
    "    else:\n",
    "        raise Exception('TODO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2167eb9",
   "metadata": {},
   "source": [
    "## Use"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cd9de87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T16:43:21.742924Z",
     "start_time": "2022-04-14T16:43:21.709070Z"
    }
   },
   "source": [
    "# sort the imputation techniques by score\n",
    "# either select the top-k or use their fancy threshold-free approach\n",
    "\n",
    "# the resulting list of imputation techniques is\n",
    "# - (1) the recommendations of imputation techniques for a new sequence to repair\n",
    "# - (2) the promising parametrized imputation techniques to run the benchmark with to label a new cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68588809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:52:28.181664Z",
     "start_time": "2022-04-29T15:52:28.158864Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if MODE == 'use':\n",
    "    DIST_FUNC = 'cosine'\n",
    "    recommendations = get_recommendations(existing_vectors, new_profile_vector, all_cids, DIST_FUNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb672b9",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3625c513",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T16:43:21.852024Z",
     "start_time": "2022-04-14T16:43:21.817175Z"
    }
   },
   "source": [
    "# reserve a test set in 2.1\n",
    "# get the recommendations for the sequences in the test set\n",
    "# compare with their actual label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16fbb20d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T15:54:10.452811Z",
     "start_time": "2022-04-29T15:54:10.415983Z"
    }
   },
   "outputs": [],
   "source": [
    "NB_CORRECT_REC = 1\n",
    "NB_TEST = min(5000, len(test_data))\n",
    "\n",
    "K = 3 # for recall and precision @ K\n",
    "\n",
    "DIST_FUNC = 'cosine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4616a4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-29T15:54:13.351Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2708b2f48e174dd7868dc007eb6b1f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if MODE == 'eval':\n",
    "    metrics = {'prec@%i' % K: [], 'rec@%i' % K: [], 'mrr': []}\n",
    "    y_true, y_pred = [], []\n",
    "    metrics_per_category = {category: {'prec@%i' % K: [], 'rec@%i' % K: [], 'mrr': []} \n",
    "                              for category in set(Dataset.CONF['CATEGORIES'].values())}\n",
    "    y_true_per_category = {category: [] \n",
    "                             for category in set(Dataset.CONF['CATEGORIES'].values())}\n",
    "    y_pred_per_category = {category: [] \n",
    "                             for category in set(Dataset.CONF['CATEGORIES'].values())}\n",
    "    for new_profile_vector, ranked_algos, ds_name in tqdm(rdm.sample(test_data, NB_TEST), total=NB_TEST):\n",
    "        recommendations = get_recommendations(existing_vectors, new_profile_vector, all_cids, DIST_FUNC).index.tolist()\n",
    "        \n",
    "        to_rmv = 'cdrec_k2' if recommendations.index('cdrec_k2') > recommendations.index('cdrec_k3') else 'cdrec_k3'\n",
    "        del recommendations[recommendations.index(to_rmv)]\n",
    "        recommendations = ['cdrec' if 'cdrec' in x else x for x in recommendations]\n",
    "\n",
    "        #correct_labels = bench_res.sort_values(by=ERROR, ascending=True).iloc[:NB_CORRECT_REC].index.tolist()\n",
    "        correct_labels = ranked_algos.iloc[:NB_CORRECT_REC].index.tolist()\n",
    "        # rank at which each correct label is found\n",
    "        correct_labels_rank = [recommendations.index(corr_lbl)+1 for corr_lbl in correct_labels]\n",
    "        \n",
    "        y_true.append(ranked_algos.index.tolist()[0])\n",
    "        y_pred.append(recommendations[0])\n",
    "        \n",
    "        prec_at_K = sum(int(rank_i <= K) for rank_i in correct_labels_rank) / K\n",
    "        rec_at_K = sum(int(rank_i <= K) for rank_i in correct_labels_rank) / len(correct_labels)\n",
    "        mrr = 1 / correct_labels_rank[0]\n",
    "        metrics['prec@%i' % K].append(prec_at_K)\n",
    "        metrics['rec@%i' % K].append(rec_at_K)\n",
    "        metrics['mrr'].append(mrr)\n",
    "        \n",
    "        category = Dataset.CONF['CATEGORIES'][ds_name]\n",
    "        y_true_per_category[category].append(ranked_algos.index.tolist()[0])\n",
    "        y_pred_per_category[category].append(recommendations[0])\n",
    "        metrics_per_category[category]['prec@%i' % K].append(prec_at_K)\n",
    "        metrics_per_category[category]['rec@%i' % K].append(rec_at_K)\n",
    "        metrics_per_category[category]['mrr'].append(mrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d2d99",
   "metadata": {},
   "source": [
    "### Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606dec38",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-29T15:54:17.527Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if MODE == 'eval':\n",
    "    print('\\n\\n============================================================\\n', '\\033[1m Average metrics: \\033[0m')\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
    "    prec = precision_score(y_true=y_true, y_pred=y_pred, average='weighted', zero_division=0).tolist()\n",
    "    recall = recall_score(y_true=y_true, y_pred=y_pred, average='weighted', zero_division=0).tolist()\n",
    "    f1 = f1_score(y_true=y_true, y_pred=y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    avg_prec_at_k = sum(metrics['prec@%i' % K]) / len(metrics['prec@%i' % K])\n",
    "    avg_rec_at_k = sum(metrics['rec@%i' % K]) / len(metrics['rec@%i' % K])\n",
    "    mrr = (1 / NB_TEST) * sum(metrics['mrr'])\n",
    "    print('Average precision@%i: %.2f | Average recall@%i: %.2f | Mean reciprocal rank: %.2f' % (K, avg_prec_at_k, K, avg_rec_at_k, mrr))\n",
    "    print('Accuracy: %.2f | Precision: %.2f | Recall: %.2f | F1-Score: %.2f' % (acc, prec, recall, f1))\n",
    "    \n",
    "    ConfusionMatrixDisplay.from_predictions(y_true=y_true, y_pred=y_pred, xticks_rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad6683",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-29T15:54:17.767Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if MODE == 'eval':\n",
    "    print('\\n\\n============================================================\\n', '\\033[1m Average metrics per category: \\033[0m')\n",
    "    \n",
    "    for category in metrics_per_category.keys():\n",
    "        acc = accuracy_score(y_true_per_category[category], y_pred_per_category[category], normalize=True, sample_weight=None)\n",
    "        prec = precision_score(y_true=y_true_per_category[category], y_pred=y_pred_per_category[category], average='weighted', zero_division=0).tolist()\n",
    "        recall = recall_score(y_true=y_true_per_category[category], y_pred=y_pred_per_category[category], average='weighted', zero_division=0).tolist()\n",
    "        f1 = f1_score(y_true=y_true_per_category[category], y_pred=y_pred_per_category[category], average='weighted', zero_division=0)\n",
    "\n",
    "        avg_prec_at_k = sum(metrics_per_category[category]['prec@%i' % K]) / len(metrics_per_category[category]['prec@%i' % K])\n",
    "        avg_rec_at_k = sum(metrics_per_category[category]['rec@%i' % K]) / len(metrics_per_category[category]['rec@%i' % K])\n",
    "        mrr = (1 / len(metrics_per_category[category]['mrr'])) * sum(metrics_per_category[category]['mrr'])\n",
    "        print('~~ Category: %s ~~' % category)\n",
    "        print('Average precision@%i: %.2f | Average recall@%i: %.2f | Mean reciprocal rank: %.2f' % (K, avg_prec_at_k, K, avg_rec_at_k, mrr))\n",
    "        print('Accuracy: %.2f | Precision: %.2f | Recall: %.2f | F1-Score: %.2f' % (acc, prec, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babac02d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
